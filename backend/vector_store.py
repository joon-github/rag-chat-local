# backend/vector_store.py
import os
import numpy as np
import faiss
import pickle
from sentence_transformers import SentenceTransformer

VECTOR_DIR = "vector_store"
INDEX_PATH = os.path.join(VECTOR_DIR, "index.faiss")
DOCS_PATH = os.path.join(VECTOR_DIR, "doc_store.pkl")

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model = SentenceTransformer("all-MiniLM-L6-v2")
dimension = 384
index = faiss.IndexFlatL2(dimension)
doc_store = []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì´ˆê¸° ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_index():
    global index, doc_store
    if os.path.exists(INDEX_PATH) and os.path.exists(DOCS_PATH):
        print("ğŸ—‚ ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë”© ì¤‘...")
        index = faiss.read_index(INDEX_PATH)
        with open(DOCS_PATH, "rb") as f:
            doc_store = pickle.load(f)
        print(f"âœ… ë¡œë”© ì™„ë£Œ - ë¬¸ì„œ ìˆ˜: {len(doc_store)}")
    else:
        print("ğŸ†• ìƒˆ ì¸ë±ìŠ¤ ì‹œì‘")

load_index()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¸ë±ì‹± í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def add_to_index(text: str):
    if not text.strip():
        return
    embedding = model.encode([text])[0].astype("float32")
    index.add(np.array([embedding]))
    doc_store.append(text)
    print(f"âœ… ì¸ë±ì‹± ì™„ë£Œ (ì´ ë¬¸ì„œ ìˆ˜: {len(doc_store)})")
    save_index()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²€ìƒ‰ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def search(query: str, top_k=3):
    q_embedding = model.encode([query])[0].astype("float32")
    D, I = index.search(np.array([q_embedding]), top_k)
    return [doc_store[i] for i in I[0] if i < len(doc_store)]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì €ì¥ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save_index():
    if not os.path.exists(VECTOR_DIR):
        os.makedirs(VECTOR_DIR)
    faiss.write_index(index, INDEX_PATH)
    with open(DOCS_PATH, "wb") as f:
        pickle.dump(doc_store, f)